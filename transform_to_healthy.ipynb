{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 're.Match'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def get_cooking_method(text):\n",
    "    methods = ['[Ss]aut[e√©]','[Bb]ak(e|ing)','[Ff]r(y|ied)','[Rr]oast', '[Gg]rill','[Ss]team','[Pp]oach','[Ss]immer','[Bb]roil','[Bb]lanch','brais(e|ing)','[Ss]tew']\n",
    "    print(re.search(methods[2],'fried'))\n",
    "get_cooking_method(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ounce\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = \"[60 pha].Customer(16 ounce) ...\"\n",
    "m = re.search(r\"\\(([0-9]* [A-Za-z0-9_]+)\\)\", s)\n",
    "print(m.group(1))\n",
    "# def tokenize(text):\n",
    "#     tokenized = sent_tokenize(text) \n",
    "#     for i in tokenized: \n",
    "#         wordsList = nltk.word_tokenize(i) \n",
    "\n",
    "#         tagged = nltk.pos_tag(wordsList) \n",
    "\n",
    "#         print(tagged) \n",
    "\n",
    "# tokenize('12 whole wheat lasagna noodles')\n",
    "# tokenize(' 1 pound lean ground beef')\n",
    "# tokenize(' 1 (16 ounce) package cottage cheese')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cloves\n",
      "[('2', 'CD'), ('cloves', 'NNS'), ('garlic', 'JJ'), (',', ','), ('chopped', 'VBD')]\n",
      "{'ingredient_name': 'garlic ', 'quantity': '2 ', 'measurement': 'cloves'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def is_measure_word(word):\n",
    "    quantifier1 = ['bowl','dish','pound','piece','can','box','bag','carton','jar','loaf','slice','package','dash','cube','pack','head','ear','kernel','gain','grain','stalk','spear','clove']\n",
    "    quantifier2 = ['teaspoon','tablespoon','glass','cup','pint','quart','half gallon','gallon','tank','jug','bottle','keg','shot','drop']\n",
    "    quantifier3 = ['bar','tube','container','stick','roll','ball','spool','skein','yard','meter','foot','piece','pad','roll','stick','pair']\n",
    "    for q in quantifier1:\n",
    "        if re.search(q,word):\n",
    "            return True\n",
    "    for q in quantifier2:\n",
    "        if re.search(q,word):\n",
    "            return True\n",
    "    for q in quantifier3:\n",
    "        if re.search(q,word):\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "print(is_measure_word('jars'))\n",
    "def tokenize_ingredient(text):\n",
    "    #     r = re.compile(\"([0-9]*) ([a-zA-Z]+)\")\n",
    "#     print(r.findall(text))\n",
    "    ingredient = {}\n",
    "    tokenized = sent_tokenize(text) \n",
    "    quantity = ''\n",
    "    ingredient_name = ''\n",
    "    measurement = ''\n",
    "    measure_word = False\n",
    "    ignore = False\n",
    "    for i in tokenized: \n",
    "        wordsList = nltk.word_tokenize(i) \n",
    "        tagged = nltk.pos_tag(wordsList) \n",
    "        for tag in tagged:\n",
    "            if tag[1] == 'CD' and not ignore:\n",
    "                quantity += tag[0]+' '\n",
    "                measure_word = True\n",
    "            elif tag[0]== '(':\n",
    "                m = re.search(r\"\\(([0-9]* [A-Za-z0-9_]+)\\)\", s)\n",
    "                quantity += '(' +m.group(1)+') '\n",
    "                ignore = True\n",
    "            elif tag[0] == ')':\n",
    "                ignore = False\n",
    "                measure_word = True\n",
    "            elif (tag[1] == 'NN' or tag[1]=='NNS' or tag[1]=='VBZ') and not ignore:\n",
    "                word =  tag[0]\n",
    "                print(word)\n",
    "                if measure_word:\n",
    "                    if is_measure_word(word):\n",
    "                        \n",
    "                        measurement += word\n",
    "                    measure_word = False\n",
    "                else:\n",
    "                    ingredient_name += word+' '\n",
    "            elif tag[0]==',':\n",
    "                break\n",
    "            elif not ignore:\n",
    "                ingredient_name+=tag[0]+' '\n",
    "        print(tagged) \n",
    "    ingredient['ingredient_name']=ingredient_name\n",
    "    ingredient['quantity'] = quantity\n",
    "    ingredient['measurement'] = measurement\n",
    "    \n",
    "    print(ingredient)\n",
    "\n",
    "    \n",
    "\n",
    "# tokenize_ingredient(\"12 whole wheat lasagna noodles\")\n",
    "# tokenize_ingredient('1 1/2 (25 ounce) jars tomato-basil pasta sauce')\n",
    "tokenize_ingredient(' 2 cloves garlic, chopped')\n",
    "# tokenize_ingredient(' 1 (60 ounce) package cottage cheese')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YUW72\\Desktop\\NLP\\Project2_Recipe\\script.py:25: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 25 of the file C:\\Users\\YUW72\\Desktop\\NLP\\Project2_Recipe\\script.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  page_graph = BeautifulSoup(page_html.content)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preheat oven to 350 degrees F (175 degrees C).', 'Fill a large pot with lightly salted water and bring to a rolling boil over high heat. Once the water is boiling, add the lasagna noodles a few at a time, and return to a boil. Cook the pasta uncovered, stirring occasionally, until the pasta has cooked through, but is still firm to the bite, about 10 minutes. Remove the noodles to a plate.', 'Place the ground beef into a skillet over medium heat, add the garlic, garlic powder, oregano, salt, and black pepper to the skillet. Cook the meat, chopping it into small chunks as it cooks, until no longer pink, about 10 minutes. Drain excess grease.', 'In a bowl, mix the cottage cheese, eggs, and Parmesan cheese until thoroughly combined.', 'Place 4 noodles side by side into the bottom of a 9x13-inch baking pan; top with a layer of the tomato-basil sauce, a layer of ground beef mixture, and a layer of the cottage cheese mixture. Repeat layers twice more, ending with a layer of sauce; sprinkle top with the mozzarella cheese. Cover the dish with aluminum foil.', 'Bake in the preheated oven until the casserole is bubbling and the cheese has melted, about 30 minutes. Remove foil and bake until cheese has begun to brown, about 10 more minutes. Allow to stand at least 10 minutes before serving.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YUW72\\Desktop\\NLP\\Project2_Recipe\\script.py:44: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 44 of the file C:\\Users\\YUW72\\Desktop\\NLP\\Project2_Recipe\\script.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  page_graph = BeautifulSoup(page_html.content)\n"
     ]
    }
   ],
   "source": [
    "import script\n",
    "keywords = 'meat lasagna'\n",
    "rf = script.RecipeFetcher()\n",
    "meat_lasagna = rf.search_recipes(keywords)[0]\n",
    "result = rf.scrape_recipe(meat_lasagna)\n",
    "# print(result)\n",
    "# print(result['ingredients'])\n",
    "print(result['directions'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
